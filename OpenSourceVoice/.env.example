# =============================================================================
# 100% OPEN SOURCE GPU VOICE AI - ENVIRONMENT CONFIGURATION
# =============================================================================
#
# Copy this file to .env and customize the settings for your deployment
# cp .env.example .env
#
# =============================================================================

# -----------------------------------------------------------------------------
# SERVER CONFIGURATION
# -----------------------------------------------------------------------------

# Port for the webhook server (default: 3003)
PORT=3003

# Node.js environment (development, production, test)
NODE_ENV=production

# Base URL for webhook callbacks (REQUIRED for production)
# This should be your public domain where Jambonz can reach your server
# For local testing, use localhost since Jambonz runs outside Docker network
WEBHOOK_BASE_URL=https://talk.example.com

# Optional: Enable debug logging
# DEBUG=voice-ai:*

# -----------------------------------------------------------------------------
# GPU SERVICE ENDPOINTS
# -----------------------------------------------------------------------------

# Ollama (Local LLM) Configuration
# Uses Docker service name for internal container communication
OLLAMA_URL=http://ollama:11434

# Available models (download with: docker exec ollama-container ollama pull <model>)
# - llama3.1:8b (recommended, 4.7GB, good quality/speed balance)
# - llama3.1:70b (40GB, highest quality, requires powerful GPU)  
# - phi3:mini (2.3GB, fastest, good for simple tasks)
# - mistral:7b (4.1GB, efficient alternative to Llama)
# - codellama:7b (specialized for coding tasks)
OLLAMA_MODEL=llama3.1:8b

# Coqui TTS (Text-to-Speech) Configuration
COQUI_TTS_URL=http://coqui-tts:5002

# VITS Speaker Selection (VCTK model speakers)
# Female voices: p225, p228, p229, p230, p231, p233, p236, p244, p248, p250
# Male voices: p226, p227, p232, p237, p238, p239, p240, p241, p243, p245
# Default: p225 (female, young, Southern England accent)
VITS_SPEAKER_ID=p225

# Faster-Whisper (Speech Recognition) Configuration  
FASTER_WHISPER_URL=http://faster-whisper:9000

# -----------------------------------------------------------------------------
# AUDIO PROCESSING CONFIGURATION
# -----------------------------------------------------------------------------

# Directory for storing generated audio files
AUDIO_DIR=/app/audio-cache

# Audio file cleanup (hours before deletion)
AUDIO_CACHE_HOURS=1

# Maximum conversation history to maintain (number of messages)
MAX_CONVERSATION_HISTORY=10

# -----------------------------------------------------------------------------
# PERFORMANCE TUNING
# -----------------------------------------------------------------------------

# LLM Generation Parameters
# Temperature: Controls randomness (0.0=deterministic, 1.0=creative)
LLM_TEMPERATURE=0.7

# Top-p: Nuclear sampling (0.9 recommended for conversations)
LLM_TOP_P=0.9

# Maximum tokens to generate (keep low for phone conversations)
LLM_MAX_TOKENS=100

# Request Timeouts (milliseconds)
LLM_TIMEOUT_MS=20000
TTS_TIMEOUT_MS=30000
STT_TIMEOUT_MS=8000

# -----------------------------------------------------------------------------
# OPTIONAL: FALLBACK SERVICES (NOT RECOMMENDED FOR OPEN-SOURCE MODE)
# -----------------------------------------------------------------------------

# OpenAI API Configuration (disabled by default)
# Only used as fallback if GPU services fail
# OPENAI_API_KEY=your-openai-api-key-here

# ElevenLabs TTS Configuration (disabled by default)
# Only used as fallback if Coqui TTS fails
# ELEVENLABS_API_KEY=your-elevenlabs-api-key-here
# ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM

# -----------------------------------------------------------------------------
# DOCKER COMPOSE OVERRIDES
# -----------------------------------------------------------------------------

# GPU Device Selection (if multiple GPUs available)
# CUDA_VISIBLE_DEVICES=0

# Container Memory Limits (for resource management)
# OLLAMA_MAX_MEMORY=8g
# TTS_MAX_MEMORY=4g
# WHISPER_MAX_MEMORY=2g

# -----------------------------------------------------------------------------
# MONITORING AND LOGGING
# -----------------------------------------------------------------------------

# Log Level (error, warn, info, debug)
LOG_LEVEL=info

# Enable performance metrics collection
ENABLE_METRICS=true

# Webhook request logging
LOG_WEBHOOKS=true

# Audio generation logging
LOG_AUDIO_GENERATION=true

# -----------------------------------------------------------------------------
# SECURITY CONFIGURATION
# -----------------------------------------------------------------------------

# Optional: Webhook authentication token
# If set, Jambonz must include this token in webhook requests
# WEBHOOK_AUTH_TOKEN=your-secure-random-token-here

# Optional: Rate limiting (requests per minute)
# RATE_LIMIT_RPM=100

# Optional: CORS configuration for web interface
# CORS_ORIGIN=https://your-admin-domain.com

# =============================================================================
# SETUP VERIFICATION
# =============================================================================
#
# After configuring, test your setup:
#
# 1. Start the services:
#    docker-compose up -d
#
# 2. Check service health:
#    curl http://localhost:3003/health
#
# 3. Download LLM model (if not already downloaded):
#    docker exec voice-ai-llm ollama pull llama3.1:8b
#
# 4. Test TTS generation:
#    curl "http://localhost:5002/api/tts?text=Hello%20world&speaker_id=p225"
#
# 5. Test webhook endpoint:
#    curl -X POST http://localhost:3003/webhook/call \
#      -H "Content-Type: application/json" \
#      -d '{"call_sid":"test","from":"+1234567890","to":"+1987654321"}'
#
# =============================================================================